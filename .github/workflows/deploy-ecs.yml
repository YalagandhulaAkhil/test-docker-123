name: Deploy ECS Cluster (read outputs from S3)

on:
  push:
    paths:
      - "infra/ecs.yml"
      - ".github/workflows/deploy-ecs.yml"
      - "scripts/read-s3-outputs.sh"
      - "scripts/save-outputs.sh"
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION:   ${{ secrets.AWS_REGION }}
  STACK_NAME:   ecs-cluster-stack
  VPC_STACK:    my-vpc-stack
  IAM_STACK:    my-iam-stack
  ECR_STACK:    ecr-stack
  S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}

jobs:
  deploy-ecs:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region:     ${{ env.AWS_REGION }}

    - name: Fetch stack outputs and export variables
      run: |
        set -euo pipefail
        chmod +x scripts/read-s3-outputs.sh

        echo "ðŸ”¹ Downloading outputs.txt files"
        for s in ${VPC_STACK} ${IAM_STACK} ${ECR_STACK}; do
          aws s3 cp s3://${S3_BUCKET_NAME}/${s}/outputs.txt - || echo " (missing $s)"
        done

        VPC_ID=$(S3_BUCKET_NAME=$S3_BUCKET_NAME scripts/read-s3-outputs.sh ${VPC_STACK} VpcId)
        SUBNET1=$(S3_BUCKET_NAME=$S3_BUCKET_NAME scripts/read-s3-outputs.sh ${VPC_STACK} PublicSubnet1Id)
        SUBNET2=$(S3_BUCKET_NAME=$S3_BUCKET_NAME scripts/read-s3-outputs.sh ${VPC_STACK} PublicSubnet2Id)
        PROFILE_ARN=$(S3_BUCKET_NAME=$S3_BUCKET_NAME scripts/read-s3-outputs.sh ${IAM_STACK} ECSInstanceProfileArn)
        REPO_URI=$(S3_BUCKET_NAME=$S3_BUCKET_NAME scripts/read-s3-outputs.sh ${ECR_STACK} RepoUri || true)

        echo "ðŸ”¹ Parsed:"
        echo "  VPC_ID=$VPC_ID"
        echo "  SUBNET1=$SUBNET1"
        echo "  SUBNET2=$SUBNET2"
        echo "  PROFILE_ARN=$PROFILE_ARN"

        for var in VPC_ID SUBNET1 SUBNET2 PROFILE_ARN; do
          [[ -z "${!var}" ]] && { echo "âŒ Missing $var"; exit 1; }
        done

        echo "VPC_ID=$VPC_ID"           >> $GITHUB_ENV
        echo "SUBNET1=$SUBNET1"         >> $GITHUB_ENV
        echo "SUBNET2=$SUBNET2"         >> $GITHUB_ENV
        echo "PROFILE_ARN=$PROFILE_ARN" >> $GITHUB_ENV

    - name: Deploy ECS CloudFormation stack
      id: deploy
      run: |
        set -e
        aws cloudformation deploy \
          --stack-name ${STACK_NAME} \
          --template-file infra/ecs.yml \
          --parameter-overrides \
              PVpcId=${VPC_ID} \
              PSubnet1=${SUBNET1} \
              PSubnet2=${SUBNET2} \
              PInstanceProfileArn=${PROFILE_ARN} \
          --capabilities CAPABILITY_NAMED_IAM \
          --no-fail-on-empty-changeset

    - name: Dump CFN events on failure
      if: failure() && steps.deploy.outcome == 'failure'
      run: |
        aws cloudformation describe-stack-events \
          --stack-name ${STACK_NAME} --max-items 30 --output table

    - name: Save ECS outputs and push to S3
      if: success()
      run: |
        chmod +x scripts/save-outputs.sh
        S3_BUCKET_NAME=${S3_BUCKET_NAME} \
        ./scripts/save-outputs.sh ${STACK_NAME}

    - name: Upload outputs.txt artifact
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: ecs-outputs
        path: infra/outputs.txt
