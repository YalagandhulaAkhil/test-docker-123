name: Deploy ECS Cluster

on:
  push:
    paths:
      - "infra/ecs.yml"
      - ".github/workflows/deploy-ecs.yml"
      - "scripts/save-outputs.sh"
      - "scripts/read-s3-outputs.sh"
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION:   ${{ secrets.AWS_REGION }}
  STACK_NAME:   ecs-cluster-stack
  VPC_STACK:    my-vpc-stack
  IAM_STACK:    my-iam-stack
  S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}

jobs:
  deploy-ecs:
    runs-on: ubuntu-latest

    steps:
    # 1 â€“ Checkout
    - uses: actions/checkout@v4

    # 2 â€“ AWS credentials (OIDC)
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region:     ${{ env.AWS_REGION }}

    # 3 â€“ Download & parse outputs
    - name: Read outputs from previous stacks
      run: |
        set -euo pipefail

        echo "ðŸ“¥ Downloading outputs.txt files"
        aws s3 cp s3://${S3_BUCKET_NAME}/${VPC_STACK}/outputs.txt vpc_outputs.txt
        aws s3 cp s3://${S3_BUCKET_NAME}/${IAM_STACK}/outputs.txt iam_outputs.txt

        echo "ðŸ” vpc_outputs.txt"; cat vpc_outputs.txt
        echo "ðŸ” iam_outputs.txt"; cat iam_outputs.txt

        VPC_ID=$(grep '^VpcId=' vpc_outputs.txt | cut -d '=' -f2 || echo "")
        SUBNET1=$(grep '^PublicSubnet1Id=' vpc_outputs.txt | cut -d '=' -f2 || echo "")
        SUBNET2=$(grep '^PublicSubnet2Id=' vpc_outputs.txt | cut -d '=' -f2 || echo "")
        PROFILE_ARN=$(grep '^ECSInstanceProfileArn=' iam_outputs.txt | cut -d '=' -f2 || echo "")

        for var in VPC_ID SUBNET1 SUBNET2 PROFILE_ARN; do
          if [[ -z "${!var}" ]]; then
            echo "âŒ Missing or empty value for $var"
            exit 1
          fi
        done

        echo "âœ… Extracted:"
        echo "  VPC_ID=$VPC_ID"
        echo "  SUBNET1=$SUBNET1"
        echo "  SUBNET2=$SUBNET2"
        echo "  PROFILE_ARN=$PROFILE_ARN"

        echo "VPC_ID=$VPC_ID"           >> $GITHUB_ENV
        echo "SUBNET1=$SUBNET1"         >> $GITHUB_ENV
        echo "SUBNET2=$SUBNET2"         >> $GITHUB_ENV
        echo "PROFILE_ARN=$PROFILE_ARN" >> $GITHUB_ENV

    # 4 â€“ Deploy cluster
    - name: Deploy ECS CloudFormation stack
      id: deploy
      run: |
        set -e
        SubnetCSV="${SUBNET1},${SUBNET2}"
        aws cloudformation deploy \
          --stack-name ${STACK_NAME} \
          --template-file infra/ecs.yml \
          --parameter-overrides \
              ECSClusterName=html-ecs \
              VpcId=${VPC_ID} \
              SubnetIds="${SubnetCSV}" \
              Ec2InstanceProfileArn=${PROFILE_ARN} \
              KeyName=${{ secrets.AAA }} \
          --capabilities CAPABILITY_NAMED_IAM \
          --no-fail-on-empty-changeset

    # 5 â€“ Events on failure
    - name: Dump CFN events on failure
      if: failure() && steps.deploy.outcome == 'failure'
      run: |
        aws cloudformation describe-stack-events \
          --stack-name ${STACK_NAME} --max-items 30 --output table

    # 6 â€“ Store ECS outputs
    - name: Save ECS outputs to S3
      if: success()
      run: |
        chmod +x scripts/save-outputs.sh
        S3_BUCKET_NAME=${{ env.S3_BUCKET_NAME }} \
        ./scripts/save-outputs.sh ${STACK_NAME}

    # 7 â€“ Upload artifact
    - name: Upload outputs.txt artifact
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: ecs-outputs
        path: infra/outputs.txt
